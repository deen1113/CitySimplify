{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1b16f9-d9c7-4176-a9e8-6a21de2d0396",
   "metadata": {},
   "source": [
    "# City Simplify\n",
    "I wrote the code for this RAG model following along with this tutorial by pixegami on Youtube:\n",
    "https://youtu.be/2TJxpyO3ei4?si=HUa42Kv2Zxxmx9sk \n",
    "\n",
    "The requirements are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa120a-f0df-44b3-b552-6101185b4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install uvicorn pydantic fastapi python-dotenv langchainhub chromadb langchain pypdf langchain_ollama langchain_aws langchain_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22410abd-92b3-4e66-90db-0b30e6cc7999",
   "metadata": {},
   "source": [
    "Securely storing API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d192c-9151-49ad-96ad-ca9501d914c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY_CREDITS = {os.getenv(\"API_KEY\"): 20}\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f256095-b450-4a3a-be0e-271c7217ae9e",
   "metadata": {},
   "source": [
    "## Setting Up API\n",
    "Here, I use fastAPI to generate a post to the server. I use an API key in order to control who can use the API, and how many times they can use it using a credit system the reduces with each call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05fa7c-c9b7-4b45-a848-99a198be3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def verify_api_key(x_api_key: str = Header(none)):\n",
    "    credits = API_KEY_CREDITS.get(x_api_key, 0)\n",
    "    if credits <= 0:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key or no credits\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\n",
    "    API_KEY_CREDITS[x_api_key] -= 1\n",
    "\n",
    "@app.post(\"/chatbot\")\n",
    "async def chatbot(request: Request):\n",
    "    data = await request.json()\n",
    "    query = rag_query(data.get(\"query\"))\n",
    "    response = {\"message\": f\"Received query: {query}\"}\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c163a6-caaa-4e01-a13d-9530d75470e9",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ea37a-4dda-48c7-ac27-9b88e2480c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf5d2d-21ca-47f5-b3b3-2998b1cb16c2",
   "metadata": {},
   "source": [
    "## Text Splitter\n",
    "The documents by themselves are too large, so we split them into smaller chunks to make them easier to use.\n",
    "We will aslo give each chunk a unique ID, which will make adding more chunks to our vector database easier, allowing us to add and remove information without having to create an entirely new database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289458fb-b39d-4ab8-bafa-99f35e7c72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df20a-2fe6-437b-8890-ca81064bcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "    \n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "    \n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee531b6-097e-421b-863f-ac4211fc4724",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "Using Ollama embeddings, we now generate the embeddings the RAG model will use to create queries.\n",
    "The defined function will be used when we create the database itself and when we actually want to query the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9867-3c36-407d-adf2-879f9f5a06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    # embeddings = BedrockEmbeddings(credentials_profile_name=\"default\", region_name=\"us-east-1\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949e20a-390e-403c-b256-f473dafba111",
   "metadata": {},
   "source": [
    "## Creating the Database\n",
    "Using ChromaDB, we can now use the embedding function to create a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173bd39-0845-4b12-b991-f61b8de15591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory=\"chroma\",\n",
    "    embedding_function=get_embedding()\n",
    ")\n",
    "\n",
    "# Adding or updating documents\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "existing_items = db.get(include=[])\n",
    "existing_ids = set(existing_items[\"ids\"])\n",
    "print(f\"Number of existing docs in DB:{len(existing_ids)}\")\n",
    "\n",
    "# Only adding chunks that are not in the database\n",
    "\n",
    "new_chunks = []\n",
    "for chunk in chunks_with_ids:\n",
    "    if chunk.metadata[\"id\"] not in existing_ids:\n",
    "        new_chunks.append(chunk)\n",
    "\n",
    "if len(new_chunks):\n",
    "    print(f\"Adding new documents: {len(new_chunks)}\")\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "    db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    db.persist()\n",
    "else:\n",
    "    print(\"No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dafbc8-ffa9-4513-a0ba-c17e34d86d9b",
   "metadata": {},
   "source": [
    "## Setting up the model and Query\n",
    "Using Ollama, we can locally call a model the answer questions based on a prompt, context, and question. While this won't be quite how the finished product will work, it is a good way to test outputs with the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2a709-1780-4bce-b7a9-231294568a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def rag_query(query_text: str):\n",
    "    db = Chroma(\n",
    "    persist_directory=\"chroma\",\n",
    "    embedding_function=get_embedding()\n",
    "    )\n",
    "\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = OllamaLLM(model=\"llama3.2\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b7f48-8050-4371-9fcf-490c7b1c4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing query so far\n",
    "rag_query(\"How should one turn go in Catan?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
